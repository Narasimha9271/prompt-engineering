## What are LLMs?

-   LLMs, or Language Learning Models, are advanced **Artificial Intelligence models** specifically designed for understanding and generate human-like text based on the input they receive.
-   These models are typically based on deep learning architectures, such as **Transformers**, and are trained on massive amounts of text data from various sources to acquire a deep understanding of the nuances and complexities of language.
-   They can accomplish state-of-the-art performance in various Natural Language Processing (NLP) tasks such as language translation, text summarization, sentiment analysis, and question-answering, among others.
-   These models are trained on vast amounts of text data and are capable of understanding and generating human-like responses.
-   LLMs represent a significant **advancement in the field of NLP**, offering the potential to revolutionize how machines understand and interact with human language.
-   As an example, OpenAI’s GPT-3 is a prominent LLM that has gained significant attention due to its capability to generate high-quality text and perform a variety of language tasks with minimal fine-tuning.

![imm](https://roadmap.sh/guides/llms.png)

-   OpenAI has been a major contributor to this space in the past few years with their models and research. However, there are other players in the market as well e.g. Meta with their OPT, OPT-IML and LLaMA models, Google released FLAN-T5 and BERT, StableLM by Stability AI, Alpaca at Stanford and there are many other opensource models as well.

### Training an LLM Model

S-1. **Data Collection** -> from various sources such as Wikipedia, news articles, books, websites etc
S-2: **Training** -> The data then goes through a training pipeline where it is cleaned and preprocessed before being fed into the model for training.
S-3: **Evaluation** -> The final step is to evaluate the performance of the model to see how well it performs on various tasks

-   The output from the training Pipeline is an LLM model which is simply the parameters or weights which capture the knowledge learned during the training process.
-   These parameters or weights are typically serialized and stored in a file, which can then be loaded into any application that requires language processing capabilities e.g. text generation, question answering, language processing etc.

**Types of LLMs**
On a high level, LLMs can be categorized into two types i.e. Base LLMs and Instruction tuned LLMs

### Base LLMs

-   Base LLMs are the LLMs which are designed to **predict the next word** based on the training data.
-   They are not designed to answer questions, carry out conversations or help solve problems.
-   For example, if you give a base LLM the sentence “In this book about LLMs, we will discuss”, it might complete this sentence and give you “In this book about LLMs, we will discuss what LLMs are, how they work, and how you can leverage them in your applications.

### Instruction tuned LLMs

-   Instruction Tuned LLMs, instead of trying to autocomplete your text, try to follow the given instructions using the data that they have been trained on.
-   For example, if you input the sentence “What are LLMs?” it will use the data that it is trained on and try to answer the question. Similarly, if you input “What are some famous social networks?” it will try to answer the question instead of giving you a random answer.

**Instruction Tuned LLMs are built on top of Base LLMs:**

```
Instruction Tuned LLMs = Base LLMs + Further Tuning + RLHF(Reinforcement Learning with Human Feedback”)
```

### Conclusion

-   LLMs are a powerful tool that can be used to solve a wide range of language-related tasks.
-   LLMs have the potential to revolutionize the way we interact with computers and make our lives easier.
