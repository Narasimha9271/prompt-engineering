# Pitfalls of LLMs

LLMs are extremely powerful, but they are by no means perfect. There are many pitfalls that you should be aware of when using them.

### To avoid them follow below suggestions:

-   To mitigate **Model Guessing Your Intentions**, make your prompts more explicit or ask the model to think step-by-step before providing a final answer.
-   To avoid result in completely different or inconsistent responses, Ensure that your prompts are well-phrased and clear to minimize confusion.
-   One way to deal with **Model Generating Plausible but Incorrect Answers** is by adding a step for the model to verify the accuracy of its response or by prompting the model to provide evidence or a source for the given information.
-   To avoid **Verbose or Overly Technical Responses**, explicitly guide the model by making your prompt more specific, asking for a simpler response, or requesting a particular format.
-   To encourage the model to seek clarification, you can prepend your prompt with **“If the question is unclear, please ask for clarification.”**
-   Sometimes, LLMs might not complete all parts of a multi-part task or might only focus on one aspect of it. To avoid this, consider breaking the task into smaller, more manageable sub-tasks or ensure that each part of the task is clearly identified in the prompt.

## Citing Sources

LLMs for the most part cannot accurately cite sources. This is because they do not have access to the Internet, and do not exactly remember where their information came from. They will frequently generate sources that look good, but are entirely inaccurate.

Strategies like search **augmented LLMs** (LLMs that can search the Internet and other sources) can often fix this problem though.

## Bias

LLMs are often biased towards generating stereotypical responses. Even with safe guards in place, they will sometimes say sexist/racist/homophobic things. Be careful when using LLMs in consumer-facing applications, and also be careful when using them in research (they can generate biased results).

## Hallucinations

LLMs will frequently generate falsehoods when asked a question that they do not know the answer to. Sometimes they will state that they do not know the answer, but much of the time they will confidently give a wrong answer.

### Causes of Hallucinations

1. **Inherent limitations:** The training data for the LMs are massive, yet they still cannot contain the entire knowledge about the world. As a result, LMs have inherent limitations in handling certain facts or details, which leads to hallucinations in the generated text.

2. **Training data biases:** If the training data contains biases or errors, it may lead to hallucinations in the output as LMs learn from the data they’ve been exposed to.

3. **Token-based scoring:** The default behavior of many LMs, like GPT models, is to generate text based on token probabilities. Sometimes this can lead to high-probability tokens being selected even if it doesn’t make sense with the given prompt.

### Mitigating Hallucinations

To reduce the occurrence of hallucinations in the generated text, consider the following strategies:

1. **Specify instructions:** Make the prompt more explicit with clear details and constraints. This can help guide the model to generate more accurate and coherent responses.
2. **Step-by-step approach:**
3. **Validating and filtering:** Develop post-processing steps to validate and filter the generated text based on specific criteria or rules to minimize the prevalence of hallucinations in the output.

# Math

LLMs are often bad at math. They have difficulty solving simple math problems, and they are often unable to solve more complex math problems.

# Prompt Hacking

Prompt hacking is a term used to describe a situation where a model, specifically a language model, is tricked or manipulated into generating outputs that violate safety guidelines or are off-topic. This could include content that’s harmful, offensive, or not relevant to the prompt.

There are a few common techniques employed by users to attempt “prompt hacking,” such as:

1. **Manipulating keywords:** Users may introduce specific keywords or phrases that are linked to controversial, inappropriate, or harmful content in order to trick the model into generating unsafe outputs.
2. **Playing with grammar:** Users could purposely use poor grammar, spelling, or punctuation to confuse the model and elicit responses that might not be detected by safety mitigations.
3. **Asking leading questions:** Users can try to manipulate the model by asking highly biased or loaded questions, hoping to get a similar response from the model.
